{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415bab59",
   "metadata": {},
   "source": [
    "# CSE152B - Spring 2023: Homework 1\n",
    "## Computing Resources\n",
    "\n",
    "You should be assigned an account that has access to GPU clusters on https://datahub.ucsd.edu/. Steps to set up the environment:\n",
    "- Login with your UCSD credentials;\n",
    "- Launch a GPU instance with (1 GPU, 8 CPU, 16G RAM). This will lead you to a Jupyter Notebook GUI, where you can browse files, launch a terminal with bash environment via (upper-right) New -> Terminal.\n",
    "- You can also access the container with command line from your local terminal:\n",
    "    - `ssh {your ucsd id}@dsmlp-login.ucsd.edu` # use your UCSD credentials\n",
    "    - get active container via `kubectl get pods`\n",
    "    - attach to the pod via `kubesh {pod name you got from above}`\n",
    "    - then you will be in the bash environment inside the container, identical to the terminal launched from Jupyter Notebook.\n",
    "    \n",
    "## Prepare the notebook and code\n",
    "Once you are in the bash environment from the terminal, pull the code for HW1 by:\n",
    "\n",
    "```bash\n",
    "cd ~\n",
    "git clone --recurse-submodules -j8 https://github.com/Vishal-V/cse152b-hw1-release-sp23.git\n",
    "cd cse152b-hw1-release-sp23\n",
    "cd pytorch-superpoint/datasets\n",
    "cd ..\n",
    "```\n",
    "You may either directly isntall the dependencies or use a conda env. If you wish to directly install the dependencies without a virtual environment, run these lines:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "pip install -r requirements_torch.txt\n",
    "```\n",
    "\n",
    "If you wish to use a conda environment, prepare the environment only for the first time:\n",
    "```bash\n",
    "conda init bash # initialize the enviroment\n",
    "source ~/.bashrc\n",
    "```\n",
    "Install the requried packages using pip in the conda environment (this avoids any compatibility issues). Note that if you have a low allocated disk space, then consider directly installing the packages using pip for this homework.\n",
    "```bash\n",
    "conda create -y --name py38-sp python=3.8\n",
    "conda activate py38-sp\n",
    "conda install pip\n",
    "/home/<username>/.conda/envs/py3/bin/pip install -r requirements.txt\n",
    "/home/<username>/.conda/envs/py3/bin/pip install -r requirements_torch.txt\n",
    "```\n",
    "\n",
    "## Submission Instructions\n",
    "1. Attempt all questions.\n",
    "2. Please comment all your code adequately.\n",
    "3. Include all relevant information such as text answers, output images in notebook.\n",
    "4. **Academic integrity:** The homework must be completed individually.\n",
    "\n",
    "5. **Submission instructions:**  \n",
    " (a) Submit the notebook and its PDF version on Gradescope, via:\n",
    "     - Option 1: Ctrl + P -> Save as PDF (toggling Headers and footers, Background graphics)\n",
    "     \n",
    " (b) Rename your submission files as Lastname_Firstname.ipynb and Lastname_Firstname.pdf.  \n",
    " (c) Correctly select pages for each answer (only your answers; excluding the problem description text) on Gradescope to allow proper grading.\n",
    "\n",
    "6. **Due date:** Assignments are due Tue, May 9, by 11pm PST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ac567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to run this once to download the dataset. Consider commenting it out after running it.\n",
    "\n",
    "! wget -P pytorch-superpoint/datasets/ http://icvl.ee.ic.ac.uk/vbalnt/hpatches/hpatches-sequences-release.tar.gz\n",
    "! mkdir pytorch-superpoint/datasets/HPatches\n",
    "! tar -xzf pytorch-superpoint/datasets/hpatches-sequences-release.tar.gz -C pytorch-superpoint/datasets/\n",
    "! mv pytorch-superpoint/datasets/hpatches-sequences-release/* pytorch-superpoint/datasets/HPatches/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a51103",
   "metadata": {},
   "source": [
    "# Question 1: SuperPoint [[1](http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w9/DeTone_SuperPoint_Self-Supervised_Interest_CVPR_2018_paper.pdf)]\n",
    "\n",
    "In this question we will look at the method SuperPoint [1], for establishing correspondences between a pair of images. Similar to UCN [2] which was discussed in the lectures, SuperPoint is a deep network to learn descriptors. However, SuperPoint simultaneously learns an interest point detector. In this question, you will generalize your understanding from the lectures to learn about SuperPoint, answer a few conceptual questions, test SuperPoint with pretrained models and observe the results.\n",
    "\n",
    "In this question we will be using implementation of SuperPoint from a public repo (https://github.com/eric-yyjau/pytorch-superpoint).\n",
    "\n",
    "**Note**: All paths in this question are relative to *./pytorch-superpoint/*. All scripts/commands should be run from that path in the terminal (by first ``cd pytorch-superpoint/``).\n",
    "\n",
    "## Q1.1: Overall pipeline\n",
    "The overall pipeline of SuperPoint is given in Fig. 3 of the paper. Please read Section 3 with a focus on Section 3.1 and 3.2 and answer:\n",
    "\n",
    "(a) Why does SuperPoint use a shared encoder for interest point detection (Interest Point Decoder) and description (Descriptor Decoder)? (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c76a1c7",
   "metadata": {},
   "source": [
    "`write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd861c",
   "metadata": {},
   "source": [
    "(b) What is the motivation for the output shape of Interest Point Decoder being  $H/8 \\times W/8 \\times 64$, instead of directly outputting an $H \\times W$ heatmap from the last layer of the CNN-based decoder? (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b648e3",
   "metadata": {},
   "source": [
    "`write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b2a4f",
   "metadata": {},
   "source": [
    "## Q1.2: MagicPoint\n",
    "\n",
    "The training pipeline is described in Section 4 and 5. In Section 4, the detector part is *trained on synthetic images dataset* with pre-defined interest points, in a fully supervised fashion. The resulting interest point detector is named **MagicPoint**. You will run inference for **MagicPoint** with model weights pretrained on synthetic images.\n",
    "\n",
    "First, make sure the pretrained model in *configs/magicpoint_repeatability_heatmap.yaml* is:\n",
    "\n",
    "```bash\n",
    "pretrained: 'logs/magicpoint_synth_t2/checkpoints/superPointNet_100000_checkpoint.pth.tar'\n",
    "```\n",
    "\n",
    "Run the following code in your terminal to run the inference over 100 images from HPatches dataset and export the results, following the experiment setup in Section 7:\n",
    "\n",
    "```bash\n",
    "python export.py export_descriptor  configs/magicpoint_repeatability_heatmap.yaml magicpoint_syn_TEST_hpatches\n",
    "```\n",
    "\n",
    "Results are saved to: *logs/magicpoint_syn_TEST_hpatches/predictions/{%d}.npz*\n",
    "\n",
    "Then, run the evaluation script to load the exported results and compute various metrics:\n",
    "\n",
    "```bash\n",
    "python evaluation.py logs/magicpoint_syn_TEST_hpatches/predictions --repeatibility --outputImg --homography --plotMatching\n",
    "```\n",
    "\n",
    "The following metrics will be printed out at the end of the run, similar to what is measured in Table 4 and explained in detail in Appendix A:\n",
    "\n",
    "- Detector metrics:\n",
    "    - Rep. (repeatability)\n",
    "    - MLE (localization error)\n",
    "- Descriptor metrics:\n",
    "    - NN mAP (mean AP)\n",
    "    - M. score (matching score)\n",
    "- Homography accuracy under varying thresholds of pixel offsets (correctness_ave)\n",
    "\n",
    "Please read the paper for detailed description of the metrics. Visualizations of interest points detected for a pair of images are saved to *logs/magicpoint_syn_TEST_hpatches/predictions/repeatibility/{%d}m.png*. \n",
    "\n",
    "### **Please answer:**\n",
    "\n",
    "(a) Report the 4 metrics given above from the run. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa182b6",
   "metadata": {},
   "source": [
    "`write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d736ac",
   "metadata": {},
   "source": [
    "(b) Please explain: what is repeatibility? (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ee1db",
   "metadata": {},
   "source": [
    "`write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e58675",
   "metadata": {},
   "source": [
    "(c) Report detection visualizations for samples 2 and 5 by **MagicPoint** in the Markdown block below. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad51187",
   "metadata": {},
   "source": [
    "``change the paths and run the block to display the images``\n",
    "\n",
    "![](pytorch-superpoint/logs/magicpoint_syn_TEST_hpatches/pedictions/repeatibility3/2.png)\n",
    "![](pytorch-superpoint/logs/magicpoint_syn_TEST_hpatches/pedictions/repeatibility3/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d662ec",
   "metadata": {},
   "source": [
    "Then, you will use **SIFT** and **Harris corner detector** to run interest point detection on the same set of images:\n",
    "\n",
    "```bash\n",
    "python export_classical.py export_descriptor configs/classical_descriptors.yaml sift_TEST_hpatches --correspondence\n",
    "\n",
    "python evaluation.py logs/sift_TEST_hpatches/predictions --sift --repeatibility --outputImg --homography --plotMatching\n",
    "```\n",
    "\n",
    "Visualizations of interest points detected for a pair of images are saved to *logs/sift_TEST_hpatches/predictions/repeatibility3/{%d}_{HARRIS, SIFT}.png*. \n",
    "\n",
    "### **Please answer:**\n",
    "\n",
    "(d) Report the detection metrics (**repeatability** and **localization error**) from Harris detector from the above run. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a68a5",
   "metadata": {},
   "source": [
    "`write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f76b997",
   "metadata": {},
   "source": [
    "(e) Report detection visualizations from samples 2 and 5 by **Harris detector** in the Markdown block below. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5db0c",
   "metadata": {},
   "source": [
    "`change the paths and run the block to display the images`\n",
    "\n",
    "![](pytorch-superpoint/logs/sift_TEST_hpatches/pedictions/repeatibility3/2_HARRIS.png)\n",
    "![](pytorch-superpoint/logs/sift_TEST_hpatches/pedictions/repeatibility3/5_HARRIS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669dc31",
   "metadata": {},
   "source": [
    "(f) Does **MagicPoint** perform better than **Harris detector**? Explain why or why not. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1777b1",
   "metadata": {},
   "source": [
    "`write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1259342",
   "metadata": {},
   "source": [
    "## Q1.3: Homographic Adaptation and SuperPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b4a26",
   "metadata": {},
   "source": [
    "**MagicPoint** is trained with full supervision on synthetic dataset, where interest points are defined as mostly corners of simple geometric primitatives (Fig. 4). However as a base detector, MagicPoint will not generalize well to natural images (e.g. HPatches) because of domain gap. As a result, Homographic Adaptation is proposed (see Section 5) to create pseudo-ground truth interest points on real images, and use them to finetune MagicPoint on real images in a self-supervised paradigm. The finetuned detector is named **SuperPoint**. In this question, you will run inference of SuperPoint on HPatches, which is trained on COCO dataset.\n",
    "\n",
    "You will use the same script to run inference, except that using SuperPoint finetuned on COCO instead of MagicPoint trained on synthetic images dataset.\n",
    "\n",
    "First, change the pretrained model in *configs/magicpoint_repeatability_heatmap.yaml* via:\n",
    "\n",
    "```bash\n",
    "pretrained: 'logs/magicpoint_synth_t2/checkpoints/superPointNet_100000_checkpoint.pth.tar'\n",
    "```\n",
    "`==change to==>`\n",
    "```bash\n",
    "pretrained: 'logs/superpoint_coco_heat2_0/checkpoints/superPointNet_170000_checkpoint.pth.tar'\n",
    "```\n",
    "\n",
    "Then run inference via:\n",
    "\n",
    "```bash\n",
    "python export.py export_descriptor  configs/magicpoint_repeatability_heatmap.yaml superpoint_coco_TEST_hpatches\n",
    "\n",
    "python evaluation.py logs/superpoint_coco_TEST_hpatches/predictions --repeatibility --outputImg --homography --plotMatching\n",
    "```\n",
    "\n",
    "results are saved to: *logs/superpoint_coco_TEST_hpatches/predictions*\n",
    "\n",
    "### Please answer:\n",
    "\n",
    "(a) Report the detection metrics (**repeatability** and **localization error**) from **SuperPoint** from the above run. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672615fb",
   "metadata": {},
   "source": [
    "`write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8bdcfa",
   "metadata": {},
   "source": [
    "(b) Does **SuperPoint** perform better than **MagicPoint** and **Harris detector**? Explain why or why not. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305e0b9",
   "metadata": {},
   "source": [
    "`write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91582e4c",
   "metadata": {},
   "source": [
    "The descriptor is measured via descriptor metrics (**NN mAP** and **M. score**, see Section 7.3 and Appendix A)*.*  Additionally, detector and descriptor can be jointly evaluated via homography estimation, by computing homography from estimated correspondences, and measuring the average error of pixel offsets using the homography (see Appendix A).\n",
    "\n",
    "(c) Report and compare SuperPoint and SIFT on **NN mAP**, **M. score** and **Homography accuracy under varying thresholds**. (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ed13a",
   "metadata": {},
   "source": [
    "`write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6c99f",
   "metadata": {},
   "source": [
    "(d) Does SuperPoint perform better than SIFT on **Homography accuracy** for lower thresholds? Explain why or why not. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de856da",
   "metadata": {},
   "source": [
    "`write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5c5d54",
   "metadata": {},
   "source": [
    "## Q1.4 Try SuperPoint on your own image pairs\n",
    "In this question you are encouraged to try your own image pairs which have reasonable overlap of contents between two images. A sample image pair is given in *configs/magicpoint_repeatability_heatmap_your_images.yaml → `data[’im_path1']`, `data[’im_path2’]`*\n",
    "\n",
    "Please first run the following code to estimate and visualize correspondences of the sample pair from KITTI dataset. Visualization will be saved at *logs/superpoint_coco_TEST_yours/predictions/matching/0m.png*. If everything goes well, the image should look like:\n",
    "\n",
    "![](images/0m.png)\n",
    "\n",
    "Next, please change the image pair paths to a pair taken by yourself (under the assumption that reasonable amount of correspondences can be visually identified; also format has to be .jpg or .png). Also adjust the resize flag `data['preprocessing']['resize']` to be values propotional to your image aspect ratio and a size that fits into the gpu. You are also encourage to tweak the viaualization parameters of ratio/max number of all matches (variables `ratio_max` and `matches_max`) to make the visualization look good with reasonable amount of matches that is not too clustered.\n",
    "\n",
    "```bash\n",
    "python export.py export_descriptor  configs/magicpoint_repeatability_heatmap_your_images.yaml superpoint_coco_TEST_yours\n",
    "\n",
    "python evaluation.py logs/superpoint_coco_TEST_yours/predictions --outputImg --plotMatchingOnly\n",
    "```\n",
    "\n",
    "(a) Report your visualizations below. (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ab9e7",
   "metadata": {},
   "source": [
    "``change the paths and run the block to display the image``\n",
    "![](pytorch-superpoint/logs/superpoint_coco_TEST_yours/predictions/matching/0m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c471281",
   "metadata": {},
   "source": [
    "(b) **Bonus:** You are encouraged to take a second pair where it is more ‘difficult’ to establish abundant correspondences than the first set above. Explain how you created a scenario where correspondence is more challenging. Report the results again and explain your observation on the differences between results for the two pairs of images. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd652e",
   "metadata": {},
   "source": [
    "``change the paths and run the block to display the other image``\n",
    "![](pytorch-superpoint/logs/superpoint_coco_TEST_yours/predictions/matching/0m.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c505f",
   "metadata": {},
   "source": [
    "`write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a211db22",
   "metadata": {},
   "source": [
    "# Q2: Metric Learning for Fashion Images Retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74adf114",
   "metadata": {},
   "source": [
    "In this problem you will train a CNN for learning embeddings for fashion images (e.g. clothing, shoes) with techniques from metric learning (e.g. Triplet loss, hard negative mining, distance functions). Once the network is trained, you will observe the application on similar image retrival, and evaluate design choices of the model.\n",
    "\n",
    "## Q2.1: Train a simple CNN and observe training and evaluations.\n",
    "\n",
    "Run the following code within the notebook, which includes a full pipeline of learning embeddings for fashion images. By default the model will train for 2 epochs (i.e. twice over all training samples), with cosine similarity as distance function for embeddings, triplet margin loss as discussed in lectures, and semi-hard negative mining strategy. After each epoch, the pipeline will evaluate on a held-out test set to measure the accuracy with [`accuracy_calculator.get_accuracy`](https://kevinmusgrave.github.io/pytorch-metric-learning/accuracy_calculation/).\n",
    "\n",
    "Run the following pipeline, and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-metric-learning\n",
    "!pip install faiss-gpu\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "from pytorch_metric_learning.utils import common_functions as c_f\n",
    "from pytorch_metric_learning.utils.inference import InferenceModel, MatchFinder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "### Define a network architecture ###\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(x.shape) # should be [batch_size, 9216]\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "### Training ###\n",
    "def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data)\n",
    "        indices_tuple = mining_func(embeddings, labels)\n",
    "        loss = loss_func(embeddings, labels, indices_tuple)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(\n",
    "                \"Epoch {} Iteration {}: Loss = {}, Number of mined triplets = {}\".format(\n",
    "                    epoch, batch_idx, loss, mining_func.num_triplets\n",
    "                )\n",
    "            )\n",
    "        loss_list.append(loss.item())\n",
    "    return loss_list\n",
    "\n",
    "\n",
    "### convenient function from pytorch-metric-learning ###\n",
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester()\n",
    "    return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator):\n",
    "    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n",
    "    test_embeddings, test_labels = get_all_embeddings(test_set, model)\n",
    "    train_labels = train_labels.squeeze(1)\n",
    "    test_labels = test_labels.squeeze(1)\n",
    "    print(\"Computing accuracy\")\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        test_embeddings, test_labels, train_embeddings, train_labels, False\n",
    "    )\n",
    "    print(\"Test set accuracy (Precision@1) = {:f}\".format(accuracies[\"precision_at_1\"]))\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "dataset1 = datasets.FashionMNIST(\".\", train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.FashionMNIST(\".\", train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=256, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=256)\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 2\n",
    "\n",
    "\n",
    "### pytorch-metric-learning stuff ###\n",
    "distance = distances.CosineSimilarity()\n",
    "#distance = distances.LpDistance()\n",
    "reducer = reducers.ThresholdReducer(low=0)\n",
    "\n",
    "margin_num = 0.2\n",
    "loss_func = losses.TripletMarginLoss(margin=margin_num, distance=distance, reducer=reducer)\n",
    "#loss_func = losses.ContrastiveLoss()\n",
    "#loss_func = losses.ArcFaceLoss(num_classes = 10, embedding_size = 128)\n",
    "mining_func = miners.TripletMarginMiner(\n",
    "    margin=margin_num, distance=distance, type_of_triplets=\"semihard\"\n",
    ")\n",
    "\n",
    "accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)\n",
    "### pytorch-metric-learning stuff ###\n",
    "\n",
    "loss_list_all = []\n",
    "accuracies_list = []\n",
    "accuracies = test(dataset1, dataset2, model, accuracy_calculator)\n",
    "accuracies_list.append((0, accuracies))\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss_list = train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
    "    accuracies = test(dataset1, dataset2, model, accuracy_calculator)\n",
    "    loss_list_all += loss_list\n",
    "    accuracies_list.append((len(loss_list_all), accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7a85a",
   "metadata": {},
   "source": [
    "**(a)** Report the final test set accuracy. (2 points)\n",
    "\n",
    "`write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2566e27",
   "metadata": {},
   "source": [
    "**(b)** Two variables are returned from the above code: `loss_list_all` which is a list of losses at each training iteration; `accuracies_list` which is a list of tuples for accuracies (precision) from testing after each epoch, with each tuple being `(ITERATION, {'precision_at_1': SOME NUMBER})`. You will plot [a double-y-axis figure of training losses and testing accuracies](https://stackoverflow.com/a/14762601) from those two variables, with the x-axis being the training iterations, and the left y-axis being the training losses, right y-axis being the testing accuracies. A reference figure is given below; your plot does not have to be identical to it, but should ideally include two curves for losses and accuracies with the x axis being the iters, and necessary labels & legends. (5 points)\n",
    "\n",
    "![](images/download.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ccc038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370fea8d",
   "metadata": {},
   "source": [
    "**(c)** You will then test the trained model on image retrival task, to retrieve the nearest images for a few test query images. Run the following code and submit the results. Make sure results for >=2 samples are visible in your converted PDF submission. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decision(is_match):\n",
    "    if is_match:\n",
    "        print(\"Same class\")\n",
    "    else:\n",
    "        print(\"Different class\")\n",
    "\n",
    "mean = [0.1307]\n",
    "std = [0.3081]\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-m / s for m, s in zip(mean, std)], std=[1 / s for s in std]\n",
    ")\n",
    "\n",
    "def imshow(img, figsize=(8, 4)):\n",
    "    img = inv_normalize(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "THRESHOLD = 0.7\n",
    "match_finder = MatchFinder(distance=CosineSimilarity(), threshold=THRESHOLD)\n",
    "inference_model = InferenceModel(model, match_finder=match_finder)\n",
    "labels_to_indices = c_f.get_labels_to_indices(dataset2.targets)\n",
    "\n",
    "\n",
    "# get 10 nearest neighbors for an input image\n",
    "inference_model.train_knn(dataset2)\n",
    "for label in range(0, 10):\n",
    "    img_type = labels_to_indices[label]\n",
    "    img = dataset2[img_type[0]][0].unsqueeze(0)\n",
    "    print(\"query image\")\n",
    "    imshow(torchvision.utils.make_grid(img))\n",
    "    distances, indices = inference_model.get_nearest_neighbors(img, k=10)\n",
    "    nearest_imgs = [dataset2[i][0] for i in indices.cpu()[0]]\n",
    "    print(\"nearest images\")\n",
    "    imshow(torchvision.utils.make_grid(nearest_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78031397",
   "metadata": {},
   "source": [
    "**(d)** Tweak the pipeline, by\n",
    "- Train for 20 epochs instead of 2;\n",
    "\n",
    "Re-train the model. Plot the two curves again from (b) with the retrained model in the updated setting. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab127f",
   "metadata": {},
   "source": [
    "`include the image here, by downloading  from the notebook and uploading it into the ./images folder, modify the path and run the block to reveal the image:`\n",
    "\n",
    "![](images/{your image.png})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35216336",
   "metadata": {},
   "source": [
    "**(e)** Tweak the model by:\n",
    "\n",
    "- Changing `self.conv1` to output 8 channels instead of 32; \n",
    "- Removing `self.conv2` and its activation function, and modify the input dimension of `self.fc1` from `9216` to some other number to make the layers fit together;\n",
    "- Training for 20 epochs instead of 2;\n",
    "\n",
    "1) Plot the curves (training losses and validation accuracies) again; (1 points)\n",
    "\n",
    "2) Compare with the curves from (d): What difference can you observe between results (training losses and validation accuracies) of the two models? Hint: think about convergence rates, final value, etc. (2 points)\n",
    "\n",
    "3) Explain how the change of model design lead to the observed differences. (5 points)\n",
    "<!-- 2) Do you see `overfitting` with the newest model from (e)? If yes, can you explain what it is based on the comparison between (d) and (e), and what might be the cause of `overfitting`? -->\n",
    "\n",
    "4) Copy the class of your new model in the code block below; (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52866fa7",
   "metadata": {},
   "source": [
    "1) `include the image here, by downloading from the notebook and uploading it into the ./images folder, modify the path and run the block to reveal the image:`\n",
    "\n",
    "![](images/{your image.png})\n",
    "\n",
    "2) `write your answer here`\n",
    "\n",
    "3) `write your answer here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c656da",
   "metadata": {},
   "source": [
    "**(f)** Change the margin of `losses.TripletMarginLoss` from 0.2 to 10, and re-train the ORIGINAL model (without the modification in (e)) for 2 epochs. \n",
    "\n",
    "1) Report the final test accuracy;  (2 points)\n",
    "\n",
    "2) Compare with (a): what and why is the difference? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046d804",
   "metadata": {},
   "source": [
    "1) `write your answer here`\n",
    "\n",
    "2) `write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274b6e1",
   "metadata": {},
   "source": [
    "**(g)** Change the miner property to `type_of_triplets=\"hard\"` (which is for hard nagetive mining instead of looking for semi-hard negative samples) and re-train the ORIGINAL model (without the modification in (e) and (f)) for 2 epochs.\n",
    "\n",
    "1) Report the final test accuracy; (2 points)\n",
    "\n",
    "2) Plot the curves again; (1 points)\n",
    "\n",
    "3) Compare with (a): what and why is the difference? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfbe07e",
   "metadata": {},
   "source": [
    "1) `write your answer here`\n",
    "\n",
    "2) `include the image here, by downloading  from the notebook and uploading it into the ./images folder, modify the path and run the block to reveal the image:`\n",
    "\n",
    "![](images/{your image.png})\n",
    "\n",
    "3) `write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc6934",
   "metadata": {},
   "source": [
    "# Reference\n",
    "[1] DeTone, Daniel, Tomasz Malisiewicz, and Andrew Rabinovich. [\"Superpoint: Self-supervised interest point detection and description.\"](http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w9/DeTone_SuperPoint_Self-Supervised_Interest_CVPR_2018_paper.pdf) *Proceedings of the IEEE conference on computer vision and pattern recognition workshops*\n",
    ". 2018.\n",
    "\n",
    "[2] Choy, Christopher B., et al. [\"Universal correspondence network.\"](https://proceedings.neurips.cc/paper/2016/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf) *Advances in neural information processing systems*\n",
    " 29 (2016)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
